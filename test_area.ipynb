{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infix_prefix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python random_unary_binary_tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infix_prefix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(simplify(sympify(\"log(exp(x))\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infix_prefix2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisset = set([\"2\", \"3\"])\n",
    "for e in thisset:\n",
    "    print(thisset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  random_unary_binary_tree1 import random_binary_trees\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "start = time.time()\n",
    "root = random_binary_trees(n)\n",
    "time1 = time.time()\n",
    "print(\"Generate tree:\", time1 - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "start = time.time()\n",
    "root = random_binary_trees(n)\n",
    "time1 = time.time()\n",
    "print(\"Generate tree:\", time1 - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_leaf = [1/6, 1/12, 1/12, 1/12, 1/12, 1/12, 1/12, 1/12, 1/12, 1/12, 1/12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([\"x\", \"-5\", \"-4\", \"-3\", \"-2\", \"-1\", \"1\", \"2\", \"3\", \"4\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "  \n",
    "a = np.array([\"hello world\"]) \n",
    "  \n",
    "print(\"type is: \",type(a)) \n",
    "print(\"dtype is: \",a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympify(\"log(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.parsing.sympy_parser import parse_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_expr(\"log(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"test.txt\", \"w+\")\n",
    "for i in range(5):\n",
    "    string = str(i) + \"\\n\"\n",
    "    file.writelines(string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 1\n",
    "while i<=10:\n",
    "    n = random.randint(1, 15)\n",
    "    print(n)\n",
    "    root = random_binary_trees(n)\n",
    "    print(\"generated\")\n",
    "    result = traverse_unary_binary_prefix(root)\n",
    "    if 'x' not in result:\n",
    "        continue\n",
    "    print(\"prefix:\", result)    \n",
    "    result_infix = prefix_to_infix(result) \n",
    "    result_expr = \" \".join(result_infix)\n",
    "    result_expr = parse_expr_timeout(result_expr)\n",
    "    if result_expr==None:\n",
    "        continue\n",
    "    result_simp = simplify_timeout(result_expr)\n",
    "    if result_simp == \"Error\":\n",
    "        continue\n",
    "    print(\"infix:\", result_simp)\n",
    "    result_simp_prefix = infix_to_prefix(result_simp)\n",
    "    if result_simp_prefix == False:\n",
    "        continue\n",
    "    print(\"pass 1\") \n",
    "    try:\n",
    "        expr_diff = diff(result_simp)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    #print(expr_diff)\n",
    "    expression = simplify_timeout(expr_diff)\n",
    "    print(\"expression infix:\", expression)\n",
    "    expression_prefix = infix_to_prefix(expression)\n",
    "    if expression_prefix == False:\n",
    "        continue\n",
    "    print(\"expression prefix:\", expression_prefix)    \n",
    "    expression_prefix = \" \".join(expression_prefix)\n",
    "    result_simp_prefix = \" \".join(result_simp_prefix)\n",
    "    sequence.append(expression_prefix + \"\\t\" + result_simp_prefix + \"\\n\")\n",
    "    print(\"added\")\n",
    "    i += 1\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.newaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = tf.constant([], shape=(2, 3))\n",
    "a2 = tf.ones((2, 3))\n",
    "tf.concat([a2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_layer, num_heads, layer, training):\n",
    "    #multi-head self attention \n",
    "    for i in range(num_heads):\n",
    "        q_i = layers.Dense(depth, name=\"layer\" + str(layer) + \"_\" + \"en_query\" + str(i))(input_layer)\n",
    "        k_i = layers.Dense(depth, name=\"layer\" + str(layer) + \"_\" + \"en_key\" + str(i))(input_layer)\n",
    "        v_i = layers.Dense(depth, name=\"layer\" + str(layer) + \"_\" + \"en_value\" + str(i))(input_layer)\n",
    "        self_attention_i = layers.Attention(\n",
    "            use_scale=True,\n",
    "            name = \"layer\" + str(layer) + \"_\" + \"en_self_attention\" + str(i),\n",
    "            )([q_i, v_i, k_i])\n",
    "        if i == 0:\n",
    "            concat_attention = tf.concat([self_attention_i], axis=2)\n",
    "        else:\n",
    "            concat_attention = tf.concat([concat_attention, self_attention_i], axis=2)      \n",
    "    self_attention = layers.Dense(d_model, name=\"layer\" + str(layer) + \"_\" + \"en_concat_attention\")(concat_attention)\n",
    "    self_attenton = layers.Dropout(rate=0.1)(self_attention, training=training)\n",
    "    \n",
    "    #layer norm1\n",
    "    layernorm1 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"en_layer_norm1\")(self_attention + input_layer)\n",
    "    \n",
    "    # feed forward\n",
    "    ffn1 = tf.keras.layers.Dense(dff, activation='relu')(layernorm1)\n",
    "    ffn2 = tf.keras.layers.Dense(d_model, name=\"layer\" + str(layer) + \"_\" + \"en_feed_forward\")(ffn1)\n",
    "    ffn2 = layers.Dropout(rate=0.1)(ffn2, training=training)\n",
    "    \n",
    "    #layer norm2\n",
    "    layernorm2 = layers.LayerNormalization(epsilon=1e-6, name=\"layer\" + str(layer) + \"_\" + \"en_layer_norm2\")(ffn2+layernorm1)\n",
    "    return layernorm2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Symbol('x', real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.exceptions import RayTimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-19 15:02:46,102\tINFO resource_spec.py:212 -- Starting Ray with 5.42 GiB memory available for workers and up to 2.72 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-19 15:02:46,354\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "def simplify_timeout(expr):\n",
    " \n",
    "    result = None\n",
    "    \n",
    "    @ray.remote\n",
    "    def f(expr):\n",
    "        try:\n",
    "            return simplify(expr)\n",
    "        except:\n",
    "             return \"Error\"\n",
    "\n",
    "    obj_id = f.remote(expr)\n",
    "    try:\n",
    "        return ray.get(obj_id, timeout=3)\n",
    "    except RayTimeoutError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 + x + log(tanh(x) + 4) + 1\n",
      "x**2 + x + log(tanh(x) + 4) + 1\n"
     ]
    }
   ],
   "source": [
    "print(simplify_timeout(1+x+x**2+log(tanh(x)+4)))\n",
    "print(simplify_timeout(1+x+x**2+log(tanh(x)+4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'sympy.calculus.util.AccumulationBounds'>\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(type(AccumBounds(0, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.29833364 0.39729702 0.45133793]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print(x)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = math_ops.logical_not(tf.constant([False, False, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_ops.cast(padding_mask, dtype=K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask2(size):\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    #mask = tf.cast(mask, tf.int32)\n",
    "    mask = tf.cast(mask, tf.bool)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(3, 3)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "print(x.shape)\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print(temp.shape)\n",
    "temp2 = create_look_ahead_mask2(x.shape[1])\n",
    "print(temp2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf. cast(1 - tf.cast(tf.math.equal(seq, 0), tf.int32), tf.bool)\n",
    "  \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "y = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[7 6 0 0 1]\n",
      " [1 2 3 0 0]\n",
      " [0 0 0 4 5]], shape=(3, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ True  True False False  True]\n",
      " [ True  True  True False False]\n",
      " [False False False  True  True]], shape=(3, 5), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "z = array_ops.expand_dims(y, axis=-2)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ True False False]]\n",
      "\n",
      " [[ True  True False]]\n",
      "\n",
      " [[ True  True  True]]], shape=(3, 1, 3), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "zz = array_ops.expand_dims(temp2, axis=-2)\n",
    "print(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RayConnectionError",
     "evalue": "Ray has not been started yet. You can start Ray with 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ef5b0445f63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Try to put this list that recursively contains itself in the object store.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(value, weakref)\u001b[0m\n\u001b[1;32m   1537\u001b[0m     \"\"\"\n\u001b[1;32m   1538\u001b[0m     \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mprofiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ray.put\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOCAL_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mcheck_connected\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             raise RayConnectionError(\"Ray has not been started yet. You can \"\n\u001b[0m\u001b[1;32m    212\u001b[0m                                      \"start Ray with 'ray.init()'.\")\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayConnectionError\u001b[0m: Ray has not been started yet. You can start Ray with 'ray.init()'."
     ]
    }
   ],
   "source": [
    "l = []\n",
    "l.append(l)\n",
    "\n",
    "# Try to put this list that recursively contains itself in the object store.\n",
    "ray.put(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 10:02:07,183\tINFO resource_spec.py:212 -- Starting Ray with 4.88 GiB memory available for workers and up to 2.45 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-03-21 10:02:07,426\tINFO services.py:1123 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "class Foo(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = []\n",
    "        self.value = self.value.append(value)\n",
    "\n",
    "def custom_serializer(obj):\n",
    "    return obj.value\n",
    "\n",
    "def custom_deserializer(value):\n",
    "    object = Foo(value)\n",
    "   # object.value = value\n",
    "    return object\n",
    "\n",
    "ray.register_custom_serializer(Foo, serializer=custom_serializer, deserializer=custom_deserializer)\n",
    "\n",
    "object_id = ray.put(Foo(100))\n",
    "print(ray.get(object_id).value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29045\n"
     ]
    }
   ],
   "source": [
    "ls dataset | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds1 = tf.data.Dataset.list_files(\"./data_generation/*.txt\", shuffle=None, seed=None) \n",
    "list_ds2 = tf.data.Dataset.list_files(\"./dataset/*.txt\", shuffle=None, seed=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = list_ds1.concatenate(list_ds2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'./data_generation/bwd800_18.txt'\n",
      "b'./data_generation/bwd800_46.txt'\n",
      "b'./data_generation/bwd800_47.txt'\n",
      "b'./data_generation/bwd800_85.txt'\n",
      "b'./data_generation/bwd800_90.txt'\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    line = tf.io.read_file(file_path)\n",
    "    line1 = tf.strings.split(line, sep='\\n')\n",
    "    line2 = tf.strings.split(line1, sep='\\t')\n",
    "    return line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list_ds2.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (None, None), types: tf.string>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = tf.data.experimental.cardinality(lines).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29045"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines.take(2):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tf.io.read_file(b'./data_generation/bwd800_18.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.TextLineDataset(\"./data_generation/bwd800_0.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'* 72 * pow + 63 * 8 * x + * -3 exp 1 * 2 x -2 + -63 * 16 pow x 2\\t+ -4 * -3 pow + * -1 exp 1 + * / 2 3 x * / 21 8 pow x -1 -1'\n",
      "b'* -4 * pow x -1 * pow + -25 * 4 pow + log x sin 3 2 / -1 2 pow acosh + * / -2 5 log x * / -2 5 sin 3 -2\\t+ -5 * -2 pow acosh + * / -2 5 log x * / -2 5 sin 3 -1'\n",
      "b'0\\t+ 5 exp * 4 pow + 2 * -1 asin 2 -1'\n",
      "b'* pow cos x -2 acosh + * / 1 2 atanh 3 * / -1 2 tanh 3\\t* + -3 tan x acosh + * / 1 2 atanh 3 * / -1 2 tanh 3'\n",
      "b'* 2 * pow + 1 * -1 pow + * -1 tan + 1 * -1 tan 4 tan + * / 5 2 x asinh 3 2 / -1 2 + / 5 2 * / 5 2 pow tan + * / 5 2 x asinh 3 2\\t+ -3 * -2 acos + * -1 tan + 1 * -1 tan 4 tan + * / 5 2 x asinh 3'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset_test.take(5):\n",
    "  print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_list = os.listdir(\"./data/dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [os.path.join(\".\", \"data\", \"dataset\", path) for path in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30466"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1884f6ff4311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "a = ['a', 'b', 'a']\n",
    "b = [0, 1, 0]\n",
    "c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b =[['c']]\n",
    "a.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['c']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_dataset.py --cpu 8 --num 256 --n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/dataset | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U ray-0.9.0.dev0-cp36-cp36m-macosx_10_13_intel.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 0.025600863289563108]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 0.03384250043584397]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 0.03384250043584397]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    " \n",
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    "\tsequences = [[list(), 1.0]]\n",
    "\t# walk over each step in sequence\n",
    "\tfor row in data:       \n",
    "\t\tall_candidates = list()\n",
    "\t\t# expand each current candidate\n",
    "\t\tfor i in range(len(sequences)):\n",
    "\t\t\tseq, score = sequences[i]           \n",
    "\t\t\tfor j in range(len(row)):\n",
    "\t\t\t\tcandidate = [seq + [j], score * -log(row[j])]\n",
    "\t\t\t\tall_candidates.append(candidate)           \n",
    "\t\t# order all candidates by score\n",
    "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "\t\t# select k best\n",
    "\t\tsequences = ordered[:k]\n",
    "\treturn sequences\n",
    " \n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)\n",
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "#print result\n",
    "for seq in result:\n",
    "\tprint(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for row in data:\n",
    "    i +=1\n",
    "print(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.expand_dims(np.expand_dims(data[0], axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(1, 0), dtype=int64, numpy=array([], shape=(1, 0), dtype=int64)>],\n",
       " <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.69314698]])>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.ctc_decode(data1, input_length=np.ones(data1.shape[0])*data1.shape[1], greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0), dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(tf.keras.backend.ctc_decode(data1, input_length=np.ones(data1.shape[0])*data1.shape[1], greedy=True)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(data1.shape[0])*data1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length=np.ones(data1.shape[0])*data1.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(1, 0), dtype=int64, numpy=array([], shape=(1, 0), dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[3]])>,\n",
       "  <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[2]])>],\n",
       " <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[-1.09861242, -1.32175592, -1.60943791]])>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.ctc_decode(data1, input_length=np.ones(data1.shape[0])*data1.shape[1], greedy=False, beam_width=3, top_paths=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose] name: transpose/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8214d61c7690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctc_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mctc_decode\u001b[0;34m(y_pred, input_length, greedy, beam_width, top_paths)\u001b[0m\n\u001b[1;32m   5794\u001b[0m               \u001b[0mthe\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5795\u001b[0m   \"\"\"\n\u001b[0;32m-> 5796\u001b[0;31m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5797\u001b[0m   \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  10528\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10529\u001b[0m         return transpose_eager_fallback(\n\u001b[0;32m> 10530\u001b[0;31m             x, perm, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  10531\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10532\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose_eager_fallback\u001b[0;34m(x, perm, name, ctx)\u001b[0m\n\u001b[1;32m  10555\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tperm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10556\u001b[0m   _result = _execute.execute(b\"Transpose\", 1, inputs=_inputs_flat,\n\u001b[0;32m> 10557\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m  10558\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10559\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/opt/anaconda3/envs/symbolic_math/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose] name: transpose/"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.ctc_decode(data, input_length=(data.shape[0], ), greedy=False, beam_width=4, top_paths=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic_math",
   "language": "python",
   "name": "symbolic_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
